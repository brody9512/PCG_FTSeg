# PCG_FTSeg: Enhancement of Phonocardiogram Segmentation using Convolutional Neural Networks with Fourier transform Module

<p align="center"><img width="100%" src="./image/model_figure.png" /></p>

## ğŸ’¡ Highlights
+ Introduce an enhanced U-Net architecture integrated with Convolutional Fourier Transform (CF) modules that fuse time-domain convolution and frequency-domain analysis via FFT and iFFT, enabling precise extraction of both temporal and spectral features for accurate PCG segmentation.
+ Employ a dual CF module strategyâ€”applied consecutively in both encoder and decoder layersâ€”to robustly differentiate between S1 and S2 heart sounds and background noise, thereby significantly improving segmentation performance even in the presence of heart murmurs.
+ Validate the proposed approach on multiple datasets (internal PhysioNet 2016 and external PhysioNet 2022 and AMC), demonstrating superior performance with an average F1 score of 97.64% for S1 and S2 segmentation compared to state-of-the-art methods such as LR-HSMM, LSTM, and BiLSTM.

## ğŸ“„ Paper
This repository provides the official implementation code for the paper: **"Enhancement of Phonocardiogram Segmentation using Convolutional Neural Networks with Fourier transform Module."**

Authors: [Changhyun Park](https://github.com/brody9512), Keewon Shin, Jinew Seo, Hyunseok Lim, Gyeong Hoon Kim, Woo-
Young Seo, Sung-Hoon Kim, Namkug Kim

[MI2RL LAB](https://www.mi2rl.co/)

## ğŸ’¾ Requirements
- numpy >= 1.21.0
- scipy >= 1.7.0
- torch >= 1.10.0
- pytorch-lightning >= 1.6.0
- monai >= 0.9.0
- scikit-learn >= 1.0.0
- neurokit2 >= 0.1.9
- livelossplot >= 0.5.5
- matplotlib >= 3.4.0
- librosa >= 0.10.2
- natsort >= 8.4.0

## ğŸ§± Repository Structure
```
â”œâ”€â”€ config.py 
â”œâ”€â”€ dataset.py 
â”œâ”€â”€ main.py 
â”œâ”€â”€ model.py 
â”œâ”€â”€ modules.py 
â”œâ”€â”€ utils.py 
â”œâ”€â”€ image 
â”‚ â””â”€â”€ model_figure.png 
â”œâ”€â”€ preprocessing 
â”‚ â””â”€â”€ signal_2016_and_AMC.py 
â”‚ â””â”€â”€ signal_2022.py
```

## ğŸ—‚ï¸ Datasets
- Internal Dataset (Train/Validation):
    - Uses the publicly available [PhysioNet 2016](https://physionet.org/content/challenge-2016/1.0.0/) dataset
- External Datasets (Test):
    - Public [PhysioNet 2022](https://physionet.org/content/challenge-2022/1.0.0/) dataset
    - Asan Medical Center (AMC) data; however, due to security regulations in Korea, the AMC data cannot be disclosed

## ğŸ“œ Script Example
```bash
python main.py 
    --gpu 0 
    --ver ['appropriate version identifier each time you run the script'] 
    --toler 40 
    --featureLength 6144 
    --target_sr 1000 
    --lowpass 20_200 
    --year 2016 
    --fft 
    --twice
```

## ğŸ¯ Results: 
Comparison of Methods on MITHSDB Benchmark (Tolerance Window = 40 ms)
### Internal (PhysioNet 2016)
| Method                 | Se (%)         | P+ (%)         | F1 S1 (%)       | F1 S2 (%)       | F1 (%)         |
|------------------------|---------------:|---------------:|----------------:|----------------:|---------------:|
| **LR-HSMM**           | 95.6           | 94.4           | -               | -               | 95.0           |
| **LSTM**              | 93.50          | 92.47          | 94.27           | 91.67           | 92.95          |
| **BiLSTM**            | 94.87          | 94.53          | 95.58           | 93.86           | 94.93          |
| **U-Net**             | 91.72Â±0.10*    | 92.06Â±0.14*    | 94.72Â±0.11*     | 88.82Â±0.06*     | 91.77Â±0.02*    |
| **U-Net + CFT (Ours)**| 97.67Â±0.03     | 97.64Â±0.15     | 98.82Â±0.16      | 96.46Â±0.03      | 97.64Â±0.09     |
### External - 1 (PhysioNet 2022)
| Method                 | Se (%)         | P+ (%)         | F1 S1 (%)       | F1 S2 (%)       | F1 (%)         |
|------------------------|---------------:|---------------:|----------------:|----------------:|---------------:|
| **U-Net**             | 89.74Â±0.27*    | 91.94Â±0.20*    | 90.81Â±0.47*     | 90.54Â±0.00*     | 90.67Â±0.24*    |
| **U-Net + CFT (Ours)**| 94.62Â±0.26     | 95.57Â±0.01     | 94.97Â±0.14      | 95.11Â±0.12      | 95.04Â±0.13     |
### External - 2 (AMC)
| Method                 | Se (%)         | P+ (%)         | F1 S1 (%)       | F1 S2 (%)       | F1 (%)         |
|------------------------|---------------:|---------------:|----------------:|----------------:|---------------:|
| **U-Net**             | 97.83Â±0.30*    | 98.33Â±0.38*    | 98.04Â±0.42*     | 98.07Â±0.27*     | 98.06Â±0.35*    |
| **U-Net + CFT (Ours)**| 99.18Â±0.13     | 99.16Â±0.14     | 99.12Â±0.07      | 99.21Â±0.02      | 99.16Â±0.14     |
> **Note:** Data are shown as mean Â± standard deviation.  
> `* p-value < 0.05 (paired t-test)`, comparing the proposed model vs. the other methods.  
> The best results are highlighted in **bold**.  
> - **BiLSTM**: Bidirectional long short-term memory  
> - **CF**: Convolutional Fourier transform  
> - **LR-HSMM**: Logistic regression-hidden semi-Markov model  
> - **LSTM**: Long short-term memory  
> - **MITHSDB**: Massachusetts Institute of Technology Heart Sound Database  

## ğŸ“ Citation
If you use this code or find it useful in your research, please cite our paper:
```bibitex
@article{pcg_ftseg,
  title={Enhancement of Phonocardiogram Segmentation using Convolutional Neural Networks with Fourier transform Module},
  author={Changhyun Park and Keewon Shin and Jinew Seo and Hyunseok Lim and Gyeong Hoon Kim and Woo-Young Seo and Sung-Hoon Kim and Namkug Kim},
  year={2025}
}
```

##ğŸ™‹ğŸ»â€â™‚ï¸ Acknowledgements
We would like to thank **Junseong Lee** ([@junjslee](https://github.com/junjslee)) for his notable contributions to refactoring the code structure and setting up this GitHub repository.

